---
title: "Easy tool calls with ellmer and chatlas"
format: html
execute:
  eval: false
---

## What is tool calling?

Large language models (LLMs) are powerful but lack access to external data or systems unless you explicitly connect them. For example, if you ask an LLM what your schedule looks like next week, it will respond that it doesn't know because it doesn't have access to your calendar.

::: {.callout-tip}
We will use [ellmer](https://ellmer.tidyverse.org/index.html) and [chatlas](https://posit-dev.github.io/chatlas/) to interact with LLMs. If you're unfamiliar with the basics of ellmer, chatlas, or working with LLM APIs, check out the packages websites and these resources:

* [Announcing ellmer: A package for interacting with Large Language Models in R](https://posit.co/blog/announcing-ellmer/)
* [Announcing chatlas: A package for interacting with Large Language Models in Python](https://posit.co/blog/announcing-chatlas/)
:::

::: {.panel-tabset} 

## Python

```{python}
import chatlas

chat = chatlas.ChatAnthropic(system_prompt="You are a helpful personal assistant.")
chat.chat("What's my schedule like next week?")
```

```
I don't have access to your personal calendar or schedule. I can't see your actual 
appointments or commitments. To know your schedule for next week, you would need to
check your own calendar, planner, or scheduling system.   
```

## R

```{r}
library(ellmer)

chat <- chat_claude(system_prompt = "You are a helpful personal assistant.")
chat$chat("What's my schedule like next week?")
```

```
I don't have access to your personal schedule. To help you with your schedule for 
next week, I would need you to share that information with me. 

You could tell me about any appointments you already have planned, or ask me for 
suggestions on how to organize your upcoming week effectively.
```
:::

If the LLM could access our calendar, however, it could easily reason about it and answer our scheduling questions. We can use **tool calling** to grant the LLM that access.

## What is tool calling? 

Tool calling lets an LLM request that specific code be run when it needs to access data or perform actions outside its own capabilities. It works like this:

1. You ask the LLM to answer a question, do a task, etc. 

2. The model realizes it needs a function (a "tool") to fulfill the request.

3. The model asks the host code to run that function.

4. The function runs, returns data, and the model uses that data in its response.

::: {.callout-tip}
For more information about tool-calling, see the Tool calling articles on the [ellmer](https://ellmer.tidyverse.org/articles/tool-calling.html) and [chatlas websites](https://posit-dev.github.io/chatlas/tool-calling.html). 
:::

Importantly, the **model itself does not execute code**. The model only requests that the caller (our R or Python script) executes the code. 

## Why use tool calling?

Tool calling gives the LLM capabilities it wouldn't otherwise have access to, including:

* Access to APIs, databases, files, etc. 
* The ability to use up-to-date or user-specific information.
* Respond more intelligently or specifically to user requests.


## Implement tool calling in ellmer or chatlas

To implement tool calling with ellmer or chatlas, you need three components:

1. **A function (or tool)** that implements your desired action.

2. A **chat object** created with ellmer or chatlas.

3. **Tool registration** so the model knows how to use your function.

While not required, you will also likely want to write a custom system prompt instructing the model when and how to use your tool.

### Calendar API example

Now, let's see it in action in our calendar example. Here is the code you need for either Python or R. Next, we'll look at each of the components in more detail. 

::: {.panel-tabset}

### Python

```{python}
# 1. Define and document the tool

def get_calendar_events(start_date: str, end_date: str) -> List[Dict[str, str]]:
    """
    Fetch Google Calendar events between two dates.

    Parameters
    ----------
    start_date : str
        The start date in "YYYY-MM-DD" format.
    end_date : str
        The end date in "YYYY-MM-DD" format.

    Returns
    -------
    List[dict]
        A list of events with 'start' and 'summary' fields.
    """
    creds = authenticate()
    service = build_calendar_service(creds)
    return query_api(service, start_date, end_date)

# 2. Initialize a chat

chat = chatlas.ChatAnthropic(system_prompt=open("prompt-calendar.md", "r").read())

# 3. Register the tool

chat.register_tool(get_calendar_events)

#  Start the chat console 
chat.console()
```

### R

```{r}
# 1. Define and document the tool

#' Gets the calendar events between start_date and end_date
#'
#' @param start_date Start date, in YYYY-MM-DD format.
#' #' @param end_date End date, in YYYY-MM-DD format.
#' @return A tibble containing the calendar events in the date range. 
get_calendar_events <- function(start_date, end_date) {

  token <- calendar_authenticate()

  events <- query_api(start_date, end_date, token)

  if (length(events) == 0) { return(tibble()) }

  tibble(
    id = events$id,
    summary = events$summary,
    start = events$start$dateTime,
    end =  events$end$dateTime
  )
}

# 2. Initialize a chat

chat <- chat_claude(system_prompt = readLines("prompt-calendar.md"))

# 3. Register the tool

chat$register_tool(tool(
  get_calendar_events,
  "Fetches calendar events between a specified start and end date.",
  start_date = type_string(
       "The start date from which to fetch calendar events. It should be a string representing the 
date in the format YYYY-MM-DD.",
       required = TRUE
  ),
  end_date = type_string(
       "The end date up to which to fetch calendar events. It should be a string representing the 
date in the format YYYY-MM-DD.",
       required = TRUE
  )
))

#  Start the chat console 
live_console(chat)
```

:::

::: {.callout-note} 
To keep the example focused, we've created helper functions for authenticating to the Google Calendar API and querying the API. You can see the full script here: [link].
:::

Let's take a look at each of the pieces.

### 1. Define the Tool Function

Tool functions are regular R or Python functions. 

::: {.panel-tabset}

### Python

```{python}
def get_calendar_events(start_date: str, end_date: str) -> List[Dict[str, str]]:
    """
    Fetch Google Calendar events between two dates.

    Parameters
    ----------
    start_date : str
        The start date in "YYYY-MM-DD" format.
    end_date : str
        The end date in "YYYY-MM-DD" format.

    Returns
    -------
    List[dict]
        A list of events with 'start' and 'summary' fields.
    """
    creds = authenticate()
    service = build_calendar_service(creds)
    return query_api(service, start_date, end_date)
```

### R

In R, use [roxygen2 comments](https://roxygen2.r-lib.org/) to document the function, just like you would for a function in an R package.

```{r}
#' Gets the calendar events between start_date and end_date
#'
#' @param start_date Start date, in YYYY-MM-DD format.
#' #' @param end_date End date, in YYYY-MM-DD format.
#' @return A tibble containing the calendar events in the date range. 
get_calendar_events <- function(start_date, end_date) {

  token <- calendar_authenticate()

  events <- query_api(start_date, end_date, token)

  if (length(events) == 0) { return(tibble()) }

  tibble(
    id = events$id,
    summary = events$summary,
    start = events$start$dateTime,
    end =  events$end$dateTime
  )
}
```

:::

The process is largely the same as writing a function for a human user. When thinking about the functions inputs and outputs, you'll need to consider:

* **What information does the LLM have access to?** Make sure the function's inputs are things you want the LLM to control and that it can provide.
* **What information do you want the LLM to have after the tool call completes?** The function should return this information.

For example, in our case, we want the LLM to be able to retrieve calendar information. Based on the user's request, the LLM can provide dates as inputs, and the function will return a list (Python) or tibble (R) of calendar events.

Because the LLM communicates through JSON, inputs and outputs must use JSON-compatible data types, like floats, lists, or simple data frames. Do not return complex objects, functions, environments, etc. 

Keep in mind that returning large data frames can consume many tokens, as each cell contributes to the token count.

You can learn more about type limitations [here](https://ellmer.tidyverse.org/articles/tool-calling.html) for ellmer and here for [chatlas](https://posit-dev.github.io/chatlas/tool-calling.html#tool-limitations). 

#### 2. Initialize a chat

Create a chat with ellmer or chatlas using your desired chat function. You can see a list of available models and their corresponding functions for ellmer [here](https://ellmer.tidyverse.org/reference/index.html#chatbots) and for chatlas [here](https://posit-dev.github.io/chatlas/reference/#chat-model-providers). 

It is also helpful to supply a system prompt that specifies when and how to use the tool, as well as example conversations and tool calls. In our calendar examples, we supply [this markdown](https://github.com/skaltman/tool-call-blog/blob/main/prompt-calendar.md) file containing our prompt.

::: {.callout-tip}
You can learn more about prompt design [here](https://posit-dev.github.io/chatlas/prompt-design.html).
:::

::: {.panel-tabset}

### Python

```{python}
chat = chatlas.ChatAnthropic(system_prompt=open("prompt-calendar.md", "r").read())
```

### R

```{r}
chat <- chat_claude(system_prompt = readLines("prompt-calendar.md"))
```

:::

#### 3. Register the tool

Finally, registering the tool allows the LLM to request the tool be called.

To register the tool, use `chat.register_tool()` in Python or `chat$register_tool()` and `tool()` in R. 

::: {.panel-tabset}

### Python

```{python}
chat.register_tool(get_calendar_events)
```

### R

In R, use `tool()`'s required `.description` argument to describe what the function does. To provide information about the function arguments, you can also provide name-value pairs that define the arguments and their types accepted by the function. You can see the available type specifications [here](https://ellmer.tidyverse.org/reference/type_boolean.html). Use `required = TRUE` if that argument is required. 


```{r}
chat$register_tool(tool(
  get_calendar_events,
  .description = "Fetches calendar events between a specified start and end date.",
  start_date = type_string(
       "The start date from which to fetch calendar events. It should be a string representing the 
date in the format YYYY-MM-DD.",
       required = TRUE
  ),
  end_date = type_string(
       "The end date up to which to fetch calendar events. It should be a string representing the 
date in the format YYYY-MM-DD.",
       required = TRUE
  )
))
```

:::

### Start chatting

The final step is just to either send an initial message to the model or start up an interactive conversation in the console (`chat.console()` in Python; `live_console(chat)` in R). 

Below, you can see our calendar tool in action.

<!-- images/chat-console-ellmer.mov --> 


### In a Shiny app

Now, let's take a look at an example you might want to embed in a Shiny app. Tool calling is especially useful in interactive contexts like Shiny apps because you might want the LLM to do things within your app -- create a novel plot, filter a datatable, update a value box, etc. -- and without tool-calling, the LLM can't really do that. Remember, they are limiting to essentially receiving and sending JSON-compatible files. Without extra work by you, they can't just work your Shiny app for you. Just like in our previous examples, you may also want the LLM in your Shiny app to have access to information that it doesn't natively have, like your current time or calendar.

These use cases all follow a similar pattern: you want the Shiny app to reactively update based on what the user says (in a chat stream or input box) but use the LLMs ability to turn vague user input ("I want to see data for the past three years") into an actionable function call.

One thing that you'll likely want to do in these Shiny tool functions that we didn't do earlier, however, is essentially call them for their side effects. Now, the function output might not be as important as them updating a reactive value. 

Let's take a look at an example app that can tell you about a database and create a plot based on the user's requests. 

## Tips and tricks

1. Build up your app or command line tool part-by-part. First, just write a function that carries out the funcionality you want and test that function normally, without even thinking about AI. When you're satisfied it works as expected, then you register the tool, write a system prompt, and test it out in a command line tool. After that works, you can embed the tool in a Shiny app. Without a step by step process, it can be difficult to pinpint where an issue is coming from. 

1. **Build incrementally.** Start by just writing and testing the core function, without involving the LLM. Once it works, register the tool, write a system prompt, and test in a simple chat console interface. Then, if you want, try embedding the tool in a Shiny app. Working step-by-step makes it easier to pinpoint issues.

2. **Debug using model responses.** If something goes wrong, you can ask the model what happened. It has access to the full error message returned by your tool function, which can help you diagnose the issue without even leaving the chat.

3. **Write a thorough system prompt.** Registering the tool provides some information about how the tool should work, but including additional information in the system prompt makes it more likely that the LLM will actually call the tool how and when you want. See the [sidebot system prompt](https://github.com/jcheng5/r-sidebot/blob/main/prompt.md) for an example of a thorough, well-structured system prompt. You may also need to "debug" or add to your system prompt if the LLM isn't using your tool how you want. 

4. **Verify that the tool is actually being called.** Sometimes, the model might simulate a response without actually triggering the tool. If the results seem off or the data seems fabricated, check that the tool function is actually being called. You may need to revise your system prompt to clarify when and how the LLM should initiate a tool call.

5. **Chain multiple tools.** LLMs can call more than one tool per turn. In our example, the LLM first calls `get_date()` to fetch the current date, and then passes that information to `get_calendar_events()`, all in one turn.


6. **Take advantage of automatic retries.** If a tool call fails, ellmer or chatlas will return the error to the model, and in many cases the model will attempt to recover and try again with new arguments.

## Debugging

Debugging tool calls is a bit different than regular debugging, since issues can come from multiple areas:

1. **The function itself.** Your function might have "traditional" bugs that throw an error or produce unexpected results.
2. **The tool registration.** Make sure you're registering the correct function, and that the argument types are specified correctly.
3. **The system prompt.** The model needs clear instructions about when and how to use your tool.

To help you diagnose problmes, chatlas and ellmer allow you to echo tool calls and their results. This shows how and when the model requested a tool call, what was returned, and any error message the tool produced.

::: {.panel-tabset} 

#### Python

Use `echo="all"` in `chat.chat()` to view tool calls, their results, and any error messages.

```{python}
def get_date():
    "Gets the current date."
    raise ValueError("Error in tool function.")

chat = chatlas.ChatAnthropic()
chat.register_tool(get_date)

chat.chat("What date is it today?", echo="all")
```

````                                                                                                               
👤 User turn:                                                   
                                                                                                                  
What date is it today?                                                                                                                                                                                                                                       
🤖 Assistant turn:                                                
                                                                                                                  
I'll try to help you find out today's date again.<< 🤖 other content >>                                           
                                                                                                                  
                                                                                                                  
 # tool request (toolu_0184fJdYdvo5C2ibvHAC4aQi)                                                                  
 get_date()                                                                                                       
                                                                                                                  
                                                                                                                  
<< 🤖 finish reason: tool_use >>                                                                                  
                                                                                                 
                                                                                                                  
👤 User turn:                                                   
                                                                                                                  
                                                                                                                  
 # tool result (toolu_0184fJdYdvo5C2ibvHAC4aQi)                                                                   
 Tool calling failed with error: 'Error in tool function.'                                                        
                                                                                                                  
🤖 Assistant turn:                                                
                                                                                                                  
I apologize for the continued technical difficulties with the date function. If you'd like me to try a different approach to help you, please let me know.                                     

<< 🤖 finish reason: end_turn >>                                                                                  

<chatlas._chat.ChatResponse at 0x11e67d1e0>
````

#### R

Use `echo = "output"` in `chat()` to echo the tool request and response.

```{r}
get_date <- function() {
  Sys.Date()
}

chat <- chat_anthropic(model = "claude-3-7-sonnet-latest")

chat$register_tool(tool(get_date, "Gets the current date."))

chat$chat("What date is it today?", echo = "output")
```

````
I can check the current date for you.
◯ [tool call] get_date()
● #> "2025-04-29"
Today's date is April 29, 2025.
````

And if something goes wrong:

```{r}
get_date <- function() {
  # Always throws an error
  stop("Error in tool function.")
}

chat <- chat_anthropic(model = "claude-3-7-sonnet-latest")

chat$register_tool(tool(get_date, "Gets the current date."))

chat$chat("What date is it today?", echo = "output")
```

````
I can check the current date for you.
◯ [tool call] get_date()
■ #> Error: Error in tool function.
I apologize for the error. It seems there was an issue with the date tool. The current date would be the date on 
your device or system. Since I don't have direct access to your local system information, I can't provide the 
exact date on your end.

If you need the current date, you can check it on your device's calendar, clock, or by looking at the date 
display on your computer or mobile device.
````

::: 